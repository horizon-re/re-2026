C:\Dvip\plantric-root\venv\lib\site-packages\huggingface_hub\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[init] Working directory: C:\Dvip\plantric-root

======================================================================
[data] Loading datasets...
======================================================================

[OK] Contextual: 3284 samples
[OK] F/NFR train: 8747 samples
[OK] F/NFR dev: 1093 samples
[OK] Synthetic non-req: 4961 samples

[combined] train=15506 dev=2579

======================================================================
[model] microsoft/deberta-v3-base
[lora] targets: ['query_proj', 'key_proj', 'value_proj']
======================================================================

Traceback (most recent call last):
  File "scripts\classifier\finetune\train_multitask_deberta.py", line 681, in <module>
    main()
  File "scripts\classifier\finetune\train_multitask_deberta.py", line 674, in main
    train_multitask(samples_train, samples_dev, args, run_root, args.model)
  File "scripts\classifier\finetune\train_multitask_deberta.py", line 320, in train_multitask
    tok = AutoTokenizer.from_pretrained(model_name)
  File "C:\Dvip\plantric-root\venv\lib\site-packages\transformers\models\auto\tokenization_auto.py", line 709, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "C:\Dvip\plantric-root\venv\lib\site-packages\transformers\tokenization_utils_base.py", line 1825, in from_pretrained
    return cls._from_pretrained(
  File "C:\Dvip\plantric-root\venv\lib\site-packages\transformers\tokenization_utils_base.py", line 1988, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "C:\Dvip\plantric-root\venv\lib\site-packages\transformers\models\deberta_v2\tokenization_deberta_v2_fast.py", line 133, in __init__
    super().__init__(
  File "C:\Dvip\plantric-root\venv\lib\site-packages\transformers\tokenization_utils_fast.py", line 114, in __init__
    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
  File "C:\Dvip\plantric-root\venv\lib\site-packages\transformers\convert_slow_tokenizer.py", line 1307, in convert_slow_tokenizer
    return converter_class(transformer_tokenizer).converted()
  File "C:\Dvip\plantric-root\venv\lib\site-packages\transformers\convert_slow_tokenizer.py", line 445, in __init__
    from .utils import sentencepiece_model_pb2 as model_pb2
  File "C:\Dvip\plantric-root\venv\lib\site-packages\transformers\utils\sentencepiece_model_pb2.py", line 91, in <module>
    _descriptor.EnumValueDescriptor(
  File "C:\Dvip\plantric-root\venv\lib\site-packages\google\protobuf\descriptor.py", line 920, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
