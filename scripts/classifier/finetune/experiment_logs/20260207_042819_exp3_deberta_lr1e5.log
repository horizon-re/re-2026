C:\Dvip\plantric-root\venv\lib\site-packages\huggingface_hub\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[init] Working directory: C:\Dvip\plantric-root

======================================================================
[data] Loading datasets...
======================================================================

[OK] Contextual: 3284 samples
[OK] F/NFR train: 8747 samples
[OK] F/NFR dev: 1093 samples
[OK] Synthetic non-req: 4961 samples

[combined] train=15506 dev=2579

======================================================================
[model] microsoft/deberta-v3-base
[lora] targets: ['query_proj', 'key_proj', 'value_proj']
======================================================================

[encoder] Embedding dimension: 768
[data] train=15506 dev=2579
[task1] req_pos=10379/15506
[task2] func_pos=5449/8747
[task3] amb_pos=245/2689

======================================================================
[train] Multi-task LoRA fine-tuning
======================================================================

[ep 01] req_f1=0.784 req_acc=0.743 | func_f1=0.800 | amb_f1=0.000
[best] req_f1=0.784 
[ep 02] req_f1=0.800 req_acc=0.768 | func_f1=0.834 | amb_f1=0.000
[best] req_f1=0.800 
[ep 03] req_f1=0.807 req_acc=0.791 | func_f1=0.850 | amb_f1=0.000
[best] req_f1=0.807 
[ep 04] req_f1=0.811 req_acc=0.797 | func_f1=0.841 | amb_f1=0.000
[best] req_f1=0.811 
[ep 05] req_f1=0.829 req_acc=0.815 | func_f1=0.854 | amb_f1=0.000
[best] req_f1=0.829 
[ep 06] req_f1=0.821 req_acc=0.813 | func_f1=0.857 | amb_f1=0.000
[ep 07] req_f1=0.840 req_acc=0.830 | func_f1=0.863 | amb_f1=0.000
[best] req_f1=0.840 
[ep 08] req_f1=0.841 req_acc=0.834 | func_f1=0.856 | amb_f1=0.000
[best] req_f1=0.841 
[ep 09] req_f1=0.852 req_acc=0.841 | func_f1=0.868 | amb_f1=0.000
[best] req_f1=0.852 
[ep 10] req_f1=0.856 req_acc=0.842 | func_f1=0.868 | amb_f1=0.000
[best] req_f1=0.856 
[done] saved -> classifier\models\multitask_deberta_lora\20260207_070644\deberta

======================================================================
[done] Model saved -> classifier\models\multitask_deberta_lora\20260207_070644\deberta
======================================================================

